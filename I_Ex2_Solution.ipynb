{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP I - Exercise 2 solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "import urllib.request # Hacer peticiones HTML\n",
    "from bs4 import BeautifulSoup # Manipular y trabajar con HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_Spam(url_email):\n",
    "    texto = obtener_texto(url_email, 'p')\n",
    "    texto = texto + obtener_texto(url_email, 'a')\n",
    "    \n",
    "    densidad = text_density(texto, True)\n",
    "    \n",
    "    palabras_spam_1 = ['comprar', 'descargar', 'descuento', 'ayuda', 'regalo', 'premio', 'bancaria', 'más']\n",
    "    palabras_spam = []\n",
    "    \n",
    "    for token in nlp(' '.join(palabras_spam_1)):\n",
    "            palabras_spam.append(token.lemma_) # Añadimos a lista\n",
    "    \n",
    "    spam_points = 0\n",
    "    \n",
    "    fila = 0\n",
    "    filas = densidad.shape[0]\n",
    "        \n",
    "    while fila < filas:\n",
    "        \n",
    "        row = densidad.iloc[fila]\n",
    "        \n",
    "        if row.loc['Palabra'] in palabras_spam:\n",
    "            \n",
    "            if float(row.loc['Densidad']) > 0.02:\n",
    "                \n",
    "                spam_points = spam_points + 1\n",
    "        \n",
    "        fila = fila +1\n",
    "    \n",
    "    result = ''\n",
    "    \n",
    "    if spam_points > 0:\n",
    "        result = 'Si es SPAM'\n",
    "    else:\n",
    "        result = 'No es SPAM'\n",
    "         \n",
    "    return result + ' Puntos Spam: ' + str(spam_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_density(content, clean):\n",
    "    \n",
    "    # 1.- Tokenizar\n",
    "    tokens = word_tokenize(content, \"spanish\")\n",
    "    # Elimnar puntuación\n",
    "    tokens_alpha = [word.lower() for word in tokens if word.isalpha()] \n",
    "    \n",
    "    # 2.- Eliminar Stopwords\n",
    "    clean_tokens = tokens_alpha[:] # Duplicamos los tokens\n",
    "    \n",
    "    for token in tokens_alpha:\n",
    "        if token in stopwords.words('spanish'):\n",
    "            clean_tokens.remove(token)\n",
    "    \n",
    "    # 3.- Función para limpiar y obtener lemas\n",
    "    cleaned_tokens = [] # Array\n",
    "    \n",
    "    if clean:\n",
    "        # Realizamos la limpieza (SPACY)\n",
    "        separator = ' ' # Siempre es un espacio\n",
    "        \n",
    "        # Recorremos cada token analizado por spacy\n",
    "        for token in nlp(separator.join(clean_tokens)):\n",
    "            cleaned_tokens.append(token.lemma_) #Añadimos a lista\n",
    "    else:\n",
    "        cleaned_tokens = clean_tokens\n",
    "        \n",
    "    # 4.- Generamos la tabla para el output\n",
    "    conteo = pd.DataFrame(columns = ['Palabra', 'Recuento', 'Densidad'])\n",
    "    \n",
    "    # 5.- Obtenemos los datos para llenar la tabla\n",
    "    freq = nltk.FreqDist(cleaned_tokens) # Usamos NLTK\n",
    "    \n",
    "    #Recorremos el análisis de NKT y frecuencia\n",
    "    for key,val in freq.items():\n",
    "        \n",
    "        # Key es el token\n",
    "        # Val es el número de repeticiones de ese token\n",
    "        \n",
    "        # En este blucle insertamos las columnas\n",
    "        dens = \"{:.2}\".format(val/len(cleaned_tokens)) # Obtenemos densidad\n",
    "        reps = \"{:.0f}\".format(val) # Obtenemos repeticiones\n",
    "        \n",
    "        fila = pd.Series([key, reps, dens], index = conteo.columns)\n",
    "        conteo = conteo.append(fila, ignore_index=True)\n",
    "        \n",
    "    result = conteo.sort_values('Densidad', ascending=False)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_texto(url_pet, elemento):\n",
    "    \n",
    "    # 1.- Hacemos la petición a la web y descargamos el HTMLt\n",
    "    request = urllib.request.urlopen(url_pet)\n",
    "    \n",
    "    # 2.- Leemos el HTML\n",
    "    html = request.read()\n",
    "    \n",
    "    # 3.- Parsear el HTML (Utilizando Beautiful Soup)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # 4.- Recuperar un elemento concreto de la web\n",
    "    parrafos = soup.find_all(elemento)\n",
    "    \n",
    "    # 5.- Crear la respuesta, obteniendo solo el texto del elemento\n",
    "    result = ''\n",
    "    for parrafo in parrafos:\n",
    "        result = result + parrafo.get_text()\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email 1: Si es SPAM Puntos Spam: 1\n",
      "Email 2: No es SPAM Puntos Spam: 0\n"
     ]
    }
   ],
   "source": [
    "email1 = 'https://vgwq.campaign-view.com/ua/viewinbrowser?od=3z76be2304fc1bc51a1bd72c445d4b63701474d74f8db12551960783c3e1a78592&rd=11c4aa5bc740ea1a&sd=11c4aa5bc74aa5db&n=11699e4c2c382ee&mrd=11c4aa5bc74aa5c5&m=1'\n",
    "email2 = 'http://view.mail.360imprimir.es/?qs=240379cda28af83ad8e10007f41fbdc1f01d1b478296dce186feaa145a49a34f23b8a8237db5b919c795c23a8f6d5674a7029b3ea6825c6d72b6ff6ce79263a31674a89eff62e90f6a1f9f8b7debe9ca'\n",
    "\n",
    "print('Email 1: ' + es_Spam(email1))\n",
    "print('Email 2: ' + es_Spam(email2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
