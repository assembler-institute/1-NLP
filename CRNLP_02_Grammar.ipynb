{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "70b34713",
      "metadata": {
        "id": "70b34713"
      },
      "source": [
        "# NLP II: Grammar in NLP\n",
        "\n",
        "You have here the resources to learn how to gramaticaly analyze texts.\n",
        "\n",
        "## Libraries and installation\n",
        "### NLTK\n",
        "\n",
        "First we need to import the NLTK library. However, we can take advantage of the library created in our previous notebook!!\n",
        "\n",
        "Go ahead and import your handmade library called `tokenization.py`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34ed8085-2e7a-4a1e-bda4-a246d5228894",
      "metadata": {
        "id": "34ed8085-2e7a-4a1e-bda4-a246d5228894"
      },
      "outputs": [],
      "source": [
        "# Type your code here:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ddadac",
      "metadata": {},
      "outputs": [],
      "source": [
        "my_phrase = 'I want a dress for a wedding'\n",
        "tokens = tokenization.tokenize(my_phrase, 'english')\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d6ebd5e-1f26-428c-8da2-0b9d51bfdb11",
      "metadata": {
        "id": "1d6ebd5e-1f26-428c-8da2-0b9d51bfdb11"
      },
      "source": [
        "**Morphological Analysis:** refers to the process of examining and identifying the smallest meaningful units (or morphemes) of a word. This can help in understanding a word's structure, its roots, prefixes, and suffixes, and can be crucial in languages with rich morphological structures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a51bce35-f286-4eb0-8251-6f182c9a1d13",
      "metadata": {
        "id": "a51bce35-f286-4eb0-8251-6f182c9a1d13",
        "outputId": "6c67fd98-907a-4e8c-831f-2458d44f842d"
      },
      "outputs": [],
      "source": [
        "import nltk # Why we need to import nltk?! Is it clear?\n",
        "from nltk.chunk.regexp import *\n",
        "\n",
        "nltk.pos_tag(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3bd9bec",
      "metadata": {},
      "source": [
        "Did the cell execute successfully? Before diving in, you may need to install certain components. But no worries, you're already well-acquainted with such installations!\n",
        "\n",
        "Upon successful execution of the cell, you'll be presented with an array of tuples. Each tuple comprises two elements:\n",
        "\n",
        "1) **Token (or Word):** The individual word or symbol from the text.\n",
        "2) **Part-of-Speech (POS) Tag:** This tag is derived from morphological analysis and indicates the grammatical role of the token in its context.\n",
        "\n",
        "Here are some commonly encountered POS tags and their meanings:\n",
        "\n",
        "* **'NN':** Noun — Refers to entities such as people, places, objects, ideas, or concepts. Examples include 'car' and 'city'.\n",
        "* **'DT':** Determiner — These are words that introduce nouns and help clarify their context. Examples are 'the', 'an', and 'this'.\n",
        "* **'VBP':** Verb, non-3rd person singular present — Represents verbs that aren't restricted to third-person singular usage. For instance, 'run' in the sentence 'They run'.\n",
        "* **'IN':** Preposition or subordinating conjunction — These words articulate spatial or temporal relations, like 'in', 'before', 'after', and 'under'.\n",
        "\n",
        "POS tagging via `nltk.pos_tag` provides invaluable insights into the grammatical roles words play within sentences. Such insights underpin numerous NLP tasks, including parsing and entity recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a3559a0-4554-4fae-a441-a208a8793fb6",
      "metadata": {
        "id": "9a3559a0-4554-4fae-a441-a208a8793fb6"
      },
      "source": [
        "**Syntactic Analysis:** often referred to as parsing, involves examining and identifying the grammatical structure of a sentence. It determines the relationships between words and how they come together to convey meaning. Through syntactic analysis, one can identify subjects, predicates, objects, and other grammatical constructs in a sentence. This information is vital for tasks like machine translation, question answering, and many other NLP applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f59631b3-0688-4739-847d-0b11537c3719",
      "metadata": {
        "id": "f59631b3-0688-4739-847d-0b11537c3719"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import treebank"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22eb88a6",
      "metadata": {},
      "source": [
        "Let us see an in-built example from the library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57582ab1",
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('treebank')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b88e95d-be6e-40ae-82ee-dedeaf8fefae",
      "metadata": {
        "id": "2b88e95d-be6e-40ae-82ee-dedeaf8fefae",
        "outputId": "87b2c336-82b9-4975-c1ba-689f9c4c8a5d"
      },
      "outputs": [],
      "source": [
        "t = treebank.parsed_sents('wsj_0001.mrg')\n",
        "t"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f67d8e20",
      "metadata": {},
      "source": [
        "Above, you can observe the tree structure representing the hierarchical relationships of the processed phrases. This visual representation breaks down the syntactic roles and relationships within the sentence.\n",
        "\n",
        "Let's visualize the structure using a tree diagram to better understand the hierarchical relationships within the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5a7a518",
      "metadata": {},
      "outputs": [],
      "source": [
        "t[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37a99a73",
      "metadata": {},
      "source": [
        "In the provided image, you can observe the 'sintagma,' or the complete phrase in the top node. Following down, the subject is denoted as 'NP-SBJ'. As we traverse down the branches, we encounter various components that finally lead to the leaves, which are the individual words (tokens) like 'Pierre', 'Vinken', and so on.\n",
        "\n",
        "In essence, this representation organizes and analyzes the different components of the sentence, highlighting their syntactic relationships.\n",
        "\n",
        "Other manner to see the tree structure is the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fa3fec2",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(t[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5789d2e5-7307-405d-a340-896f1e1a3b4c",
      "metadata": {
        "id": "5789d2e5-7307-405d-a340-896f1e1a3b4c"
      },
      "source": [
        "**Semantic Analysis:** While syntactic analysis deals with the structure and grammar of sentences, semantic analysis delves into the meaning of those sentences. One of the key tasks in semantic analysis is Named Entity Recognition (NER).\n",
        "\n",
        "Using NER, you can extract entities such as people's names, company names, geographical locations, and more, from a body of text. This is particularly useful in various NLP applications like information retrieval, question answering, and knowledge graph construction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de417c52-5f77-4b21-9f8a-7ac02537f574",
      "metadata": {
        "id": "de417c52-5f77-4b21-9f8a-7ac02537f574",
        "outputId": "69681a30-9e95-40cc-e567-4852e748c415"
      },
      "outputs": [],
      "source": [
        "entities = nltk.chunk.ne_chunk(nltk.pos_tag(tokens))\n",
        "entities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af4ce7bd",
      "metadata": {},
      "source": [
        "#### Context-Free Grammar (CFG)\n",
        "\n",
        "In natural language processing (NLP), the creation, understanding, and generation of language structures frequently necessitate the use of formal grammars. One prevalent type of formal grammar is the Context-Free Grammar (CFG).\n",
        "\n",
        "CFG operates without prior context for analyzing a phrase. It doesn't depend on previous tags or any morphological analysis. Essentially, with CFG, we start with a clean slate. We're given raw tokens, and we designate the role of each token explicitly: this token signifies X, that token represents Y, etc. By categorizing these tokens through these declarations, they collectively form higher-level structures, painting the broader picture that is the CFG.\n",
        "\n",
        "It's noteworthy that there are other grammatical structures, which rely on pre-trained tags to form definitions or categories. These are frequently employed in certain NLP contexts, and we will see them in the near future :)\n",
        "\n",
        "\n",
        "Below, there is an **example** of tree creation with a custom CFG for my phrase:\n",
        "\n",
        "\"I want a dress for a wedding\"\n",
        "\n",
        "Earlier, after performing a morphological analysis on the phrase, the output we received was:\n",
        "\n",
        "```\n",
        "[('i', 'NN'),\n",
        " ('want', 'VBP'),\n",
        " ('a', 'DT'),\n",
        " ('dress', 'NN'),\n",
        " ('for', 'IN'),\n",
        " ('a', 'DT'),\n",
        " ('wedding', 'NN')]\n",
        " ```\n",
        "\n",
        "This output gives us individual tags for each token. Using these tags, we can define our CFG rules. Each line encapsulated between the triple quotes (\"\"\") represents a rule. The order of these rules progresses from the least restrictive to the most restrictive.\n",
        "\n",
        "Rules are made of two parts: the label to be assiged first, and the rules that must accomplish secondly. \n",
        "\n",
        "Our rules will be for the more specific (basic) rules: Det, N, etc.\n",
        "\n",
        "`Det -> 'a'`\\\n",
        "`N -> 'dress' | 'wedding'` (`|` stands for or)\\\n",
        "etc.\n",
        "\n",
        "And fot the more generic rules: S, PP, etc. These broader, more composite rules illustrate how the smaller rules connect and interact.\n",
        "\n",
        "These CFG rules help in understanding the hierarchical and structural relationships between different tokens in a sentence, crucial for several NLP tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b01a840-c419-4099-aa4c-a506f646f920",
      "metadata": {
        "id": "6b01a840-c419-4099-aa4c-a506f646f920"
      },
      "outputs": [],
      "source": [
        "grammar = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NP VP\n",
        "PP -> P NP\n",
        "NP -> Det N | Det N PP | 'i'\n",
        "VP -> V NP | VP PP\n",
        "Det -> 'a'\n",
        "N -> 'dress' | 'wedding'\n",
        "V -> 'want'\n",
        "P -> 'for'\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e6329ae",
      "metadata": {},
      "source": [
        "These are all the rules needed to tag the phrase \"I want a dress for a wedding.\"\n",
        "\n",
        "**Disadvantages:** This approach to grammar is quite manual. Consider a variant of the phrase: \"I want shoes for a wedding.\" Our current grammar would be insufficient because it doesn't account for the word \"shoes.\"  :(\n",
        "\n",
        "However, such a manual approach can be useful in specific contexts, especially when there's limited variability in input phrases or when most inputs are quite similar. While this basic form of grammar isn't the most commonly used, it's foundational knowledge that's essential for anyone diving into the world of NLP.\n",
        "\n",
        "As mentioned before, for more intricate and variable phrases, grammars built around trained taggers are usually preferred. With these, a trained tagger assigns labels to words. We then construct rules based on these labels rather than the specific words themselves. This method allows us to focus on structures like S -> NP VP and PP -> P NP, without being overly concerned about individual words, the part of the code with Det -> 'a', N -> 'dress' | 'wedding', etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea39cb25",
      "metadata": {},
      "outputs": [],
      "source": [
        "entities = nltk.chunk.ne_chunk(nltk.pos_tag(tokens))\n",
        "entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cc775c3-b242-499c-9479-2ca0e23298f4",
      "metadata": {
        "id": "8cc775c3-b242-499c-9479-2ca0e23298f4"
      },
      "outputs": [],
      "source": [
        "parser = nltk.ChartParser(grammar, trace=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc06756c-a188-4497-b300-ba7e1a238f20",
      "metadata": {
        "id": "dc06756c-a188-4497-b300-ba7e1a238f20"
      },
      "outputs": [],
      "source": [
        "def parse_cfg(_tokens):\n",
        "    return parser.parse(_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6691ac3e-548f-4bbd-a7f2-a0df879e7797",
      "metadata": {
        "id": "6691ac3e-548f-4bbd-a7f2-a0df879e7797",
        "outputId": "c8ef10ab-4602-4a92-c553-b6fbc001fc88"
      },
      "outputs": [],
      "source": [
        "for tree in parse_cfg(tokens):\n",
        "    print(tree, '\\n')\n",
        "    tree.pretty_print()\n",
        "    print('------------------------------------------------------')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80a5776c",
      "metadata": {},
      "source": [
        "As you can observe from the output, there are two distinct trees. These represent two unique ways to interpret our phrase based on the established rules. Both interpretations are compatible and plausible within the constraints of the provided rules.\n",
        "\n",
        "This phenomenon is known as **structural ambiguity**. It arises when a single sequence of words or tokens can be parsed in multiple ways due to ambiguous grammatical constructs. This is a common phenomenon in natural languages and showcases the richness and complexity of human language.\n",
        "\n",
        "For instance, consider the sentence: \"I saw the man with the telescope.\" Depending on how you parse it, the meaning can be \"I used a telescope to see the man\" or \"I saw the man who had a telescope.\"\n",
        "\n",
        "In the context of CFG and NLP, handling such ambiguities is crucial. Different parse trees can lead to different semantic interpretations of a sentence, which can significantly impact tasks like information extraction, question answering, and more."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149b641b",
      "metadata": {
        "id": "149b641b"
      },
      "source": [
        "#### Custom tagger\n",
        "\n",
        "Creating a custom tagger allows you to specifically tailor the tagging process to your needs. This can be particularly useful if you're working with texts in a specialized domain, or if the standard taggers don't perform well for your specific use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f30cbcb",
      "metadata": {
        "id": "2f30cbcb"
      },
      "outputs": [],
      "source": [
        "from nltk import UnigramTagger, BigramTagger, TrigramTagger #N-Gram"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "563b0bc0",
      "metadata": {},
      "source": [
        "An **N-gram** is a contiguous sequence of *n* items (such as characters, words, or tokens) from a given text or speech. They help capture the language structure, like phrasing, from a statistical point of view.\n",
        "\n",
        "Depending on the value of *n*, the type of **N-gram** is defined:\n",
        "\n",
        "- 1-gram (or unigram): A single item, e.g., \"apple\"\n",
        "- 2-gram (or bigram): A sequence of 2 contiguous items, e.g., \"apple pie\"\n",
        "- 3-gram (or trigram): A sequence of 3 contiguous items, e.g., \"I love you\"\n",
        "- ... and so on.\n",
        "\n",
        "N-grams provide a way to convert text data into a format that can be used for statistical analysis. For instance, bigrams can capture a bit of context, making them more informative than unigrams. But, the longer the N-gram (i.e., the higher the *n*), the sparser the statistical representation will become, which can be a challenge for tasks like language modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb3802d1",
      "metadata": {
        "id": "eb3802d1"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    'The kid have a lolipop',\n",
        "    'The women bought a car',\n",
        "    'The man is cooking a chicken'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c925a57-a9e6-49b9-aa38-058fe4ea9199",
      "metadata": {
        "id": "6c925a57-a9e6-49b9-aa38-058fe4ea9199",
        "outputId": "bcd351bb-1e07-4608-b075-dbeaeb10bc4a"
      },
      "outputs": [],
      "source": [
        "corpus_tokens = []\n",
        "\n",
        "for phrase in corpus:\n",
        "    corpus_tokens.append(tokenization.tokenize(phrase))\n",
        "\n",
        "corpus_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69ac02ba",
      "metadata": {},
      "source": [
        "Now that we have our elements, N-grams require us to tag the sentences. Let's create a training set and label our sentences.\n",
        "\n",
        "[ [('token', 'tag') , ...] , ['array_with_phrase_2'] , ... ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe78437-6b21-4dd2-a2d1-08940cf2204f",
      "metadata": {
        "id": "dbe78437-6b21-4dd2-a2d1-08940cf2204f"
      },
      "outputs": [],
      "source": [
        "tagged_corpus = [\n",
        "    [('The', 'art'), ('kid', 'suj'), ('have', 'action'), ('a', 'det'), ('lolipop', 'obj')],\n",
        "    [('The', 'art'), ('women', 'suj'), ('bought', 'action'), ('a', 'det'), ('car', 'obj')],\n",
        "    [('the', 'art'), ('man', 'suj'), ('is', 'action'), ('cooking', 'action'), ('a', 'det'), ('chicken', 'obj')]\n",
        "]\n",
        "\n",
        "uni = UnigramTagger(tagged_corpus)\n",
        "bi = BigramTagger(tagged_corpus)\n",
        "tri = TrigramTagger(tagged_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d098f334-539c-43d8-93f0-fc0a52e4ddba",
      "metadata": {
        "id": "d098f334-539c-43d8-93f0-fc0a52e4ddba",
        "outputId": "5244ee49-3640-4e9b-9917-b44269098e05"
      },
      "outputs": [],
      "source": [
        "uni.tag(corpus_tokens[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1d66f0f",
      "metadata": {},
      "source": [
        "Note that I intentionally chose one of the training items to be 'the' in lowercase; otherwise, 'the' would not be recognized.\n",
        "\n",
        "**Give it a try!!!**\n",
        "\n",
        "Let us see the differences between uni, bi and tri-gram taggers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92873f7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "tri.tag(tokenization.tokenize('The granny ates a lolipop'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4701036-2120-4f0b-972d-5a52358e201f",
      "metadata": {
        "id": "f4701036-2120-4f0b-972d-5a52358e201f",
        "outputId": "bc00bb76-3475-408c-cb84-092c21a4a0ca"
      },
      "outputs": [],
      "source": [
        "uni.tag(tokenization.tokenize('The granny eats a lolipop'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9ab31c7",
      "metadata": {},
      "source": [
        "The unigram tagger, `uni` recognizes 'the', 'a' and 'lollipop' because they were part of the training set. Contrary, the words 'granny' and 'eats' are left untagged because they were not part of the training data given to the unigram tagger.\n",
        "\n",
        "To address the limitations of trigram taggers, one approach is to implement a backoff strategy. With this method, if the bigram or the trigram tagger is unable to determine a tag for a particular token, it \"falls back\" to using a unigram tagger. This hierarchical approach increases the likelihood of assigning an accurate tag, even when specific trigram or bigram contexts are not present in the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57961dd3",
      "metadata": {},
      "outputs": [],
      "source": [
        "uni = UnigramTagger(tagged_corpus)\n",
        "bi = BigramTagger(tagged_corpus, backoff=uni)\n",
        "tri = TrigramTagger(tagged_corpus, backoff=uni)\n",
        "\n",
        "tri.tag(tokenization.tokenize('The granny eats a lolipop'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "137fb08c",
      "metadata": {},
      "source": [
        "In this way, by implementing a backoff strategy, we enhance the robustness of our tagger, decreasing the instances where it cannot assign a tag (`None`). This ensures a more comprehensive tagging across various contexts, especially when encountering unfamiliar or less frequent sequences in the input text.\n",
        "\n",
        "Let's now retrain to improve our algorithm. These types of algorithms need to be trained from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b036ac06",
      "metadata": {
        "id": "b036ac06"
      },
      "outputs": [],
      "source": [
        "tagged_corpus.append(\n",
        "    [('The', 'art'),\n",
        "     ('granny', 'suj'),\n",
        "     ('eats', 'action'),\n",
        "     ('an', 'det'),\n",
        "     ('apple', 'obj')])\n",
        "\n",
        "uni = UnigramTagger(tagged_corpus)\n",
        "bi = BigramTagger(tagged_corpus, backoff=uni)\n",
        "tri = TrigramTagger(tagged_corpus, backoff=uni)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07c40e48",
      "metadata": {
        "id": "07c40e48",
        "outputId": "eaf7e98a-f904-4b54-e2bf-7e710ae9c330"
      },
      "outputs": [],
      "source": [
        "uni.tag(tokenization.tokenize('The granny eats an apple'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b68e52e9",
      "metadata": {},
      "source": [
        "#### Resources\n",
        "\n",
        "\"EAGLES\" guidelines is a set of standards for the creation and representation of linguistic resources, especially in European languages. EAGLES stands for \"Expert Advisory Group on Language Engineering Standards.\"\n",
        "\n",
        "For Spanish, in particular, EAGLES has provided a set of morphosyntactic tagset guidelines which are widely used in corpus annotation projects. The tagset can describe various linguistic properties such as gender (masculine/feminine), number (singular/plural), tense (present/past/future), mood (indicative/subjunctive/imperative), and more.\n",
        "\n",
        "https://www.cs.upc.edu/~nlp/tools/parole-sp.html\n",
        "\n",
        "See for instance 5.NOMBRES. In the examples, 'chico' is labeled as 'NCMS000'. This label comes from the table used for tagging. As you can see, it means N(ombre)C(omún)M(asculino)S(ingular)000.\n",
        "\n",
        "Easy, right?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
