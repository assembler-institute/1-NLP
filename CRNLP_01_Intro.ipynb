{"cells":[{"cell_type":"markdown","id":"1001a5da","metadata":{"id":"1001a5da"},"source":["# NLP I: Uses and operations of NLTK\n","\n","In this notebook we are going to put into practice the tokenisation of texts.\n","\n","Tokenisation is the division of text into smaller pieces. It can be tokenised by words or phrases, although it is more common to tokenise by words.\n","\n","## Libraries and installation\n","### NLTK\n","\n","First we need to import the NLTK library."]},{"cell_type":"code","execution_count":null,"id":"ae7fa635","metadata":{"id":"ae7fa635"},"outputs":[],"source":["import nltk"]},{"cell_type":"markdown","id":"_ewn2SP33tQ5","metadata":{"id":"_ewn2SP33tQ5"},"source":["If you need to install `nltk`, here is the webpage:\n","\n","https://pypi.org/project/nltk/\n","\n","If you do not need to install `nltk`, here you can find the webpage documentation:\n","\n","https://pypi.org/project/nltk/\n","\n",";P"]},{"cell_type":"markdown","id":"894c10f3","metadata":{"id":"894c10f3"},"source":["## Working with the data\n","\n","First we will load a simple sentence to work with it and see examples in a clear way."]},{"cell_type":"code","execution_count":null,"id":"d080b80f","metadata":{"id":"d080b80f"},"outputs":[],"source":["frase = 'Me he comprado un coche rojo. Ahora tenemos que encontrar un seguro de coches a todo riesgo'"]},{"cell_type":"markdown","id":"5b1dda9c","metadata":{"id":"5b1dda9c"},"source":["### Word tokenisation\n","\n","We will use the \"word tokenize\" that we have previously imported. To do this, we load the text from the web page that we obtained and cleaned up in the previous step.\n","\n","Here is a brief explanation of the commands used: \".lower()\" what we do is standardise the formatting of all the words. The \".isalpha()\" command evaluates each token as true or flase depending on whether it is a word or not. With this we discard all punctuation marks, numbers, symbols, etc. ...\n","\n","#### NLTK Word Tokenize\n","\n","We import the Word Tokenize component of the NLTK library to generate the tokens of our text.\n","\n","It is important to take into account that we will use the Spanish tokenisation in our case for the analysis of the text."]},{"cell_type":"code","execution_count":null,"id":"806bf19a","metadata":{"id":"806bf19a"},"outputs":[],"source":["from nltk.tokenize import word_tokenize"]},{"cell_type":"markdown","id":"7aa7d857","metadata":{"id":"7aa7d857"},"source":["### We get the tokens\n","\n","To get the tokens we simply use the command `word_tokenize(t,i)` where;\n","* **t** would be the text to tokenize\n","* **i** would be the language, in our case `Spanish`."]},{"cell_type":"code","execution_count":null,"id":"94a1e942","metadata":{"id":"94a1e942"},"outputs":[],"source":["def tokenize(_frase):\n","    \"\"\"\n","    Tokeniza una frase en palabras individuales, eliminando cualquier carácter no alfabético y convirtiendo\n","    todas las palabras a minúsculas.\n","\n","    Args:\n","    _frase (str): La frase que se quiere tokenizar.\n","\n","    Returns:\n","    list: Una lista de tokens alfabéticos en minúsculas.\n","    \"\"\"\n","\n","    tokens = word_tokenize(_frase, \"spanish\")\n","    tokens = [word.lower() for word in tokens if word.isalpha()]\n","\n","    return tokens"]},{"cell_type":"code","execution_count":null,"id":"816c0899","metadata":{"id":"816c0899","outputId":"973f2f33-4997-467d-aabb-08ee239b08e9"},"outputs":[],"source":["token_frase = tokenize(frase)\n","token_frase"]},{"cell_type":"markdown","id":"fd604065","metadata":{"id":"fd604065"},"source":["Some errors might arise if you don't have all the necessary resources installed.\n","\n","The NLTK library has subcomponents that are essential for various analyses. By running the following command:\n","\n","`nltk.download()`\n","\n","an execution window will launch, as illustrated in the image below:\n","\n","<div style=\"text-align:center;\">\n","<img src=\"Images/download.png\" width=\"300\">\n","</div>\n","\n","However, this method isn't the most efficient way to download resources. A quicker approach is to specify the desired subcomponent within the parentheses. For example:\n","\n","`nltk.download('module')`\n","\n","This way, you can directly download the necessary modules without navigating through the execution window."]},{"cell_type":"markdown","id":"e689134e","metadata":{"id":"e689134e"},"source":["### Stop words\n","\n","Stop words are those words that are not really relevant to our exercise, e.g. articles, conjunctions, determiners, auxiliary verbs, etc. ...\n","\n","First we must import the NLTK package **stopwords**."]},{"cell_type":"code","execution_count":null,"id":"39ef71b9","metadata":{"id":"39ef71b9"},"outputs":[],"source":["from nltk.corpus import stopwords"]},{"cell_type":"markdown","id":"70b47fca","metadata":{"id":"70b47fca"},"source":["We can easily see the words contained within stopwords by executing the following command `stopwords.words('spanish')`."]},{"cell_type":"code","execution_count":null,"id":"e6bf4b4d","metadata":{"id":"e6bf4b4d","outputId":"381b9094-805f-45ea-aa27-f120b178214f"},"outputs":[],"source":["print(f\"There are {len(stopwords.words('spanish'))} stopwords in Spanish:\", stopwords.words('spanish'))"]},{"cell_type":"markdown","id":"7f44958e","metadata":{"id":"7f44958e"},"source":["To remove a stopword from the text, simply search for it in the list."]},{"cell_type":"code","execution_count":null,"id":"3f655a27","metadata":{"id":"3f655a27"},"outputs":[],"source":["def clean_sw(_tokens, language='spanish'):\n","    \"\"\"\n","    Removes stopwords from a list of tokens based on the specified language.\n","\n","    Args:\n","    _tokens (list of str): List of tokens (words) from which the stopwords will be removed.\n","    language (str, optional): The language of the stopwords. Defaults to 'spanish'.\n","\n","    Returns:\n","    list of str: A list of tokens with the stopwords removed.\n","\n","    Note:\n","    The function uses the NLTK library's list of stopwords for the removal. Ensure that\n","    the 'stopwords' dataset from NLTK is downloaded before using this function.\n","    \"\"\"\n","    clean_tokens = _tokens[:]\n","\n","    for token in _tokens:\n","        if token in stopwords.words('spanish'):\n","            clean_tokens.remove(token)\n","\n","    return clean_tokens"]},{"cell_type":"code","execution_count":null,"id":"38e4ca4d","metadata":{"id":"38e4ca4d","outputId":"c6bffb04-9e95-4c9f-87a5-64ac1e8abab4"},"outputs":[],"source":["clean_tokens = clean_sw(token_frase)\n","clean_tokens"]},{"cell_type":"markdown","id":"53290689","metadata":{"id":"53290689"},"source":["### Stemming\n","\n","Backward derivation allows us to eliminate verb tenses, genders, plurals, ... in order to improve the counting and grouping of words in the analysed texts.\n","\n","In our case, for Spanish, we will use the **Snowball** algorithm. We will import the `SnowballStemmer` into the **nltk.stem** package."]},{"cell_type":"code","execution_count":null,"id":"6f96e7bc","metadata":{"id":"6f96e7bc"},"outputs":[],"source":["from nltk.stem import SnowballStemmer"]},{"cell_type":"markdown","id":"5cf1dc9c","metadata":{"id":"5cf1dc9c"},"source":["As this stemmer is multi-language, we will have to specify which language we want to use.\n","\n","You can consult all the available languages, along with more documentation at: https://www.nltk.org/_modules/nltk/stem/snowball.html"]},{"cell_type":"code","execution_count":null,"id":"05f092ff","metadata":{"id":"05f092ff"},"outputs":[],"source":["spanish_stemmer = SnowballStemmer('spanish')"]},{"cell_type":"markdown","id":"3bccea85","metadata":{"id":"3bccea85"},"source":["Next, we have to load the tokens without the stopWords we have previously generated to get it (you can also load any token, even if it includes stopWords)."]},{"cell_type":"code","execution_count":null,"id":"30963fec","metadata":{"id":"30963fec","outputId":"5d46825b-a7f4-49f9-ea7d-06e634c6d9c0"},"outputs":[],"source":["stem_tokens = []\n","\n","for token in clean_tokens:\n","    stem_tokens.append(spanish_stemmer.stem(token))\n","\n","stem_tokens"]},{"cell_type":"markdown","id":"1b5ebd30","metadata":{"id":"1b5ebd30"},"source":["### Lemmatisation\n","\n","Lemmatisation, by greatly simplifying its definition, allows us to obtain the original word, for example:\n","\n","* Verbs: Eating -> Eat\n","* Plurals: Tables -> Table\n","\n","With this we can make a much more optimal classification than with backward derivation.\n","\n","To do this process in Spanish we must make use of the [spaCy library](https://spacy.io/), since NLTK does not perform this process in Spanish. The installation of spaCy is very simple, you have two options to install spaCy:\n","\n","**Option 1:** In the webpage, click on \"USAGE\" and follow the instructions below \"Install spaCy\".\n","\n","- You can choose pip as Package manager.\n","- For how to choose between processing, try to always choose CPU in Hardware.\n","- In Trained pipelines, unclick \"English\" and click on \"Spanish\"\n","- Select pipeline for efficiency.\n","- Finally, copy and paste the commands and execute them\n","\n","**Option 2:** Just run the following commands in an **Anaconda Prompt** terminal:\n","* `conda install -c conda-forge spacy`.\n","* `python -m spacy download es_core_news_sm`.\n","\n","Once installed, import the library with `import spacy` and load the Spanish package with `spacy.load('es_core_news_sm)`."]},{"cell_type":"code","execution_count":null,"id":"e27daadc","metadata":{"id":"e27daadc"},"outputs":[],"source":["#pip install -U pip setuptools wheel\n","#!pip install -U spacy\n","#!python -m spacy download es_core_news_sm\n","\n","# The ouput of the last command sais:\n","# You can now load the package via spacy.load('es_core_news_sm')"]},{"cell_type":"code","execution_count":null,"id":"5a82e8db","metadata":{"id":"5a82e8db"},"outputs":[],"source":["import spacy\n","nlp = spacy.load('es_core_news_sm')"]},{"cell_type":"markdown","id":"fade1c20","metadata":{"id":"fade1c20"},"source":["Once the language has been imported and loaded, we will proceed to obtain the lemmas."]},{"cell_type":"code","execution_count":null,"id":"fbb8292e","metadata":{"id":"fbb8292e"},"outputs":[],"source":["def lematize(_tokens):\n","    lem_tokens = []\n","\n","    separator = ' '\n","\n","    for token in nlp(separator.join(_tokens)):\n","        lem_tokens.append(token.lemma_)\n","\n","    return lem_tokens"]},{"cell_type":"code","execution_count":null,"id":"6c4343e6","metadata":{"id":"6c4343e6","outputId":"5d85d5b5-b1bb-4c39-da80-d9463ac8cbcb"},"outputs":[],"source":["lematize(clean_tokens)"]},{"cell_type":"markdown","id":"8ec5295d","metadata":{"id":"8ec5295d"},"source":["**Warm-up exercise:** Can you please build the whole proces with a frase taken from google?"]},{"cell_type":"code","execution_count":null,"id":"4bdb66f3","metadata":{"id":"4bdb66f3"},"outputs":[],"source":["# Type your code here:\n","\n","sentence = ...\n"]},{"cell_type":"code","execution_count":null,"id":"bc38096e","metadata":{"id":"bc38096e","outputId":"698df054-4069-4b59-978e-aea6563eb8c3"},"outputs":[],"source":["print(lem_tokens)"]},{"cell_type":"markdown","id":"4108b1b6","metadata":{"id":"4108b1b6"},"source":["Can you see the changes when lematizing?\n","\n","**Excercice:** Suppose you do not want to keep proper nouns, surnames or words like this on your tokens.\n","\n","Create a function that `extra_clean()` your `lem_tokens` from these undesired words."]},{"cell_type":"code","execution_count":null,"id":"ed0a9fa6","metadata":{"id":"ed0a9fa6"},"outputs":[],"source":["# Complete with your code below:\n","\n","def extra_clean(_tokensIn):\n","    \"\"\"\n","    Cleans a list of tokens by removing specific unwanted tokens.\n","\n","    This function iterates through a list of tokens and removes any that are found\n","    in the predefined `_toDelete` list.\n","\n","    Args:\n","    _tokensIn (list of str): The input list of tokens to be cleaned.\n","\n","    Returns:\n","    list of str: A cleaned list of tokens with specific words removed.\n","\n","    Note:\n","    This function creates a copy of the input list to ensure the original list remains unchanged.\n","    It is important to understand that direct modifications on the input list while iterating over it\n","    can lead to unexpected behavior. By working on a copy, such issues are avoided.\n","    \"\"\"\n","\n","    _toDelete = [...Here you can put the words to delete...]\n","\n","    _tokens = _tokensIn[:] # Do you really understand why do we need a copy of the list?\n","\n","    # Type your code here:\n","    #\n","    #\n","\n","    return _tokens"]},{"cell_type":"code","execution_count":null,"id":"1f9bdf3e","metadata":{"id":"1f9bdf3e","outputId":"1fe2f992-82a8-4b50-8e85-81bfc1534ec5"},"outputs":[],"source":["final_tokens = extra_clean(lem_tokens)"]},{"cell_type":"code","execution_count":null,"id":"32a714b4","metadata":{"id":"32a714b4","outputId":"7991720e-34dc-4615-efcb-cc1369ee2766"},"outputs":[],"source":["print(final_tokens)"]},{"cell_type":"markdown","id":"8c6c94fd","metadata":{"id":"8c6c94fd"},"source":["**Excercise:** Imagine you are developing a project where specific words, such as certain proper nouns and surnames, are considered undesirable and should be treated as stopwords. Your task is to create a library that allows users to customize their list of stopwords.\n","\n","Build a library? Slow down cerebrito!!\n","\n","Let us go step by step and create a minilibrary:\n","\n","1) Create a folder in your tree directoy called 'Libraries'.\n","2) Create an empty file called `__init__.py`.\n","3) Create a file called `custom_stopwords.py` and inside, create a class called tools (this step is optional; i did it). Beneeth the methods of this class, define a function with your code from `extra_clean`. Save it and now you can use it! ;)\n","\n","\n","Don't know how to use it? Jmmm....\n","\n","Follow these steps:\n","\n","    1) Seriously!?!?!?! Import the library!!! At this point in life?!?!?!?\n","\n","        `from my_folder_recently_created import my_recently_created_library`\n","\n","    2) And then use it!!!\n","\n","        `final_tokens = my_recently_created_library.the_name_of_my_class.my_method(lem_tokens)`\n","\n","**Warning:** To ensure the changes in my library are reflected, we need to restart the kernel. This is because the kernel has already loaded the previous version of the library into memory. Upon restarting, all the loaded variables and libraries will be cleared, allowing you to load the updated version of the library the next time you import it.\n","\n","**Observation:** You can implement a library without an `__init__.py` file, but when calling it you won't be able to call sublibraries. You must run `import Libraries` directly."]},{"cell_type":"code","execution_count":null,"id":"bc89051d","metadata":{"id":"bc89051d"},"outputs":[],"source":["# Type your code here:"]},{"cell_type":"markdown","id":"daf39edc","metadata":{},"source":["**Final Excercise:** Now, inside the 'Libraties' folder, create a library called 'tokenization.py' with all the main functions written in this notebook and use it ;)\n","\n","In this manner, if you run the following cell, is equivalent to all the previous cells of the notebook :D"]},{"cell_type":"code","execution_count":null,"id":"4e1f2fef","metadata":{},"outputs":[],"source":["# Type your code here:"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":5}
